# 00 - LLM Temelleri: Temeli Anlamak ğŸ§ 

> LLM'lerin aslÄ±nda ne olduÄŸunu, nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ve neden hiÃ§bir ÅŸeyi "hatÄ±rlamadÄ±ÄŸÄ±nÄ±" Ã¶ÄŸrenin

---

## ğŸ¯ Ã–ÄŸrenme Hedefleri

Bu bÃ¶lÃ¼mÃ¼n sonunda ÅŸunlarÄ± anlayacaksÄ±nÄ±z:
- âœ… LLM'ler nedir ve ne DEÄÄ°LDÄ°R
- âœ… LLM'ler neden veri saklamaz (durumsuz hesaplama)
- âœ… Ollama'ya temel API Ã§aÄŸrÄ±larÄ± nasÄ±l yapÄ±lÄ±r
- âœ… Yapay zeka frameworklerinin arkasÄ±ndaki HTTP/REST katmanÄ±
- âœ… AkÄ±ÅŸ (streaming) ve akÄ±ÅŸ olmayan yanÄ±tlar
- âœ… Sistem promptlarÄ± ve konuÅŸma geÃ§miÅŸi
- âœ… Fine-tuning (ince ayar) ile promptlama arasÄ±ndaki fark

**Gerekli SÃ¼re:** 2-3 saat

---

## ğŸ¤” Bunu Neden AnlamanÄ±z Gerekiyor?

**Ã‡oÄŸu eÄŸitim bunu atlar ve doÄŸrudan frameworklere geÃ§er. Bu bir hatadÄ±r.**

### Bu BÃ¶lÃ¼mÃ¼ atlarsanÄ±z Ne Olur?

âŒ **LLM'lerin bir ÅŸeyleri "hatÄ±rladÄ±ÄŸÄ±nÄ±" sanÄ±rsÄ±nÄ±z** â†’ KonuÅŸmalar devam etmediÄŸinde uygulamalarÄ±nÄ±zda hatalar olur
âŒ **BaÄŸlam yÃ¶netiminin neden Ã¶nemli olduÄŸunu anlamazsÄ±nÄ±z** â†’ Token sÄ±nÄ±rlarÄ±na takÄ±lÄ±r ve nedenini merak edersiniz
âŒ **Frameworkler kafanÄ±zÄ± karÄ±ÅŸtÄ±rÄ±r** â†’ LangChain, LangGraph sihir gibi gÃ¶rÃ¼nÃ¼r
âŒ **SorunlarÄ± ayÄ±klayamazsÄ±nÄ±z** â†’ Ä°ÅŸler bozulduÄŸunda, sorunun kodunuzda mÄ± yoksa LLM'de mi olduÄŸunu bilemezsiniz
âŒ **ParanÄ±zÄ± boÅŸa harcarsÄ±nÄ±z** â†’ Her API Ã§aÄŸrÄ±sÄ±nda gereksiz baÄŸlam gÃ¶nderirsiniz

### Bunu Anlayarak Ne KazanacaksÄ±nÄ±z?

âœ… ChatGPT, Claude veya herhangi bir LLM'i Ã§aÄŸÄ±rdÄ±ÄŸÄ±nÄ±zda **tam olarak ne olduÄŸunu bilin**
âœ… **GÃ¼venle hata ayÄ±klayÄ±n** - Sadece framework'Ã¼ deÄŸil, HTTP katmanÄ±nÄ± anlayÄ±n
âœ… **Herhangi bir dilde inÅŸa edin** - Sadece REST API olduÄŸunu fark edin; JavaScript, Java, Go, ne isterseniz kullanÄ±n
âœ… **Maliyetleri optimize edin** - TokenlarÄ±, baÄŸlam pencerelerini ve bunlarÄ± nasÄ±l en aza indireceÄŸinizi anlayÄ±n
âœ… **Frameworkleri takdir edin** - LangChain'in *neden* var olduÄŸunu anlayÄ±n (bu karmaÅŸÄ±klÄ±ÄŸÄ± yÃ¶netiyor!)

### DiÄŸer Her Åeyin Temeli

Bu bÃ¶lÃ¼m ÅŸunlarÄ±n temelidir:
- **AraÃ§ Ã‡aÄŸÄ±rma** (01) - LLM'lerin araÃ§larÄ± *ne zaman* Ã§aÄŸÄ±racaÄŸÄ±nÄ± bilmesi gerekir â†’ durumsuzluÄŸu anlamayÄ± gerektirir
- **Ajan Frameworkleri** (02) - Frameworkler durumu yÃ¶netir *Ã§Ã¼nkÃ¼* LLM'ler yÃ¶netmez â†’ nedenini anlayacaksÄ±nÄ±z
- **RAG Sistemleri** (03) - LLM'ler verilerinizi bilmez â†’ eriÅŸimin (retrieval) neden gerekli olduÄŸunu anlayacaksÄ±nÄ±z
- **HafÄ±za Sistemleri** (04) - Uzun sÃ¼reli hafÄ±za vardÄ±r *Ã§Ã¼nkÃ¼* LLM'ler unutur â†’ sorunu Ã¶nce gÃ¶receksiniz

**Bunu atlayÄ±n, diÄŸer her ÅŸey kafa karÄ±ÅŸtÄ±rÄ±cÄ± bir sihir olacak. Bunda ustalaÅŸÄ±n, diÄŸer her ÅŸey mÃ¼kemmel bir anlam kazanacak.**

---

## ğŸ”‘ KRÄ°TÄ°K: Her YazÄ±lÄ±m REST API ile Yapay Zeka Kullanabilir!

**LLM'lere dalmadan Ã¶nce ÅŸunu anlayÄ±n:**

### Yapay Zeka Kullanmak Ä°Ã§in Python'a Ä°htiyacÄ±nÄ±z Yok!

LLM'lere **basit HTTP REST API Ã§aÄŸrÄ±larÄ±** ile eriÅŸilir. Bu ÅŸununla aynÄ± teknolojidir:
- Bir hava durumu API'sini Ã§aÄŸÄ±rmak
- Bir veritabanÄ± API'sinden veri Ã§ekmek
- Bir sosyal medya API'sine gÃ¶nderi yapmak

**Bu, HERHANGÄ° BÄ°R programlama dilinin yapay zeka kullanabileceÄŸi anlamÄ±na gelir:**

| Dil | Ã–rnek |
|----------|---------|
| **JavaScript** | `fetch('http://localhost:11434/api/chat', {...})` |
| **Java** | `HttpClient.newHttpClient().send(request, ...)` |
| **C#/.NET** | `await httpClient.PostAsync("...", content)` |
| **Go** | `http.Post("http://localhost:11434/api/chat", ...)` |
| **PHP** | `file_get_contents("...", false, $context)` |
| **Ruby** | `Net::HTTP.post(uri, data)` |
| **Swift** | `URLSession.shared.dataTask(with: request)` |
| **Kotlin** | `OkHttpClient().newCall(request).execute()` |
| **Rust** | `reqwest::post("...").json(&data).send()` |
| **Excel VBA Bile!** | `CreateObject("MSXML2.XMLHTTP")` |

### Bu Kurs Neden Python KullanÄ±yor?

Python kullanÄ±yoruz Ã§Ã¼nkÃ¼:
- âœ… Ã–ÄŸrenmesi ve okumasÄ± kolay (eÄŸitimler iÃ§in harika)
- âœ… MÃ¼kemmel hata ayÄ±klama araÃ§larÄ±
- âœ… Zengin yapay zeka ekosistemi (LangGraph, CrewAI, Letta)
- âœ… Yapay zeka topluluÄŸunda popÃ¼ler

**Ancak temel kavramlar HERHANGÄ° BÄ°R dilde Ã§alÄ±ÅŸÄ±r!**

### GerÃ§ek DÃ¼nya Ã–rneÄŸi: Mevcut UygulamanÄ±za Yapay Zeka Ekleyin

```javascript
// Mevcut Node.js/Express uygulamanÄ±z
app.post('/api/summarize', async (req, res) => {
  const document = req.body.document;

  // Ollama'yÄ± Ã§aÄŸÄ±rÄ±n (herhangi bir REST API'yi Ã§aÄŸÄ±rmakla aynÄ±!)
  const response = await fetch('http://localhost:11434/api/chat', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({
      model: 'qwen3:8b',
      messages: [{
        role: 'user',
        content: `Summarize this: ${document}`
      }]
    })
  });

  const data = await response.json();
  res.json({ summary: data.message.content });
});

// Ä°ÅŸte bu kadar! UygulamanÄ±za yapay zeka eklediniz!
```

### Bu Kurstaki Her Ã–rnek curl Ä°Ã§erir

Her kavram iÃ§in `curl` Ã¶rnekleri saÄŸlÄ±yoruz, bÃ¶ylece:
1. Ham HTTP katmanÄ±nÄ± anlayabilirsiniz
2. Tercih ettiÄŸiniz dilde uygulayabilirsiniz
3. Kod yazmadan test edebilirsiniz
4. API sorunlarÄ±nÄ± ayÄ±klayabilirsiniz

**Åimdi LLM'lerin aslÄ±nda ne olduÄŸunu Ã¶ÄŸrenelim!**

---

## ğŸ§  Kritik Kavram: LLM'ler Durumsuzdur (Stateless)

### LLM Nedir?

Bir LLM (BÃ¼yÃ¼k Dil Modeli) esasen ÅŸudur:
```
Girdiye dayalÄ± olarak bir sonraki tokenÄ± tahmin eden
Ã§ok geliÅŸmiÅŸ bir Ã¶rÃ¼ntÃ¼ eÅŸleÅŸtirme makinesi
```

ÅÃ¶yle dÃ¼ÅŸÃ¼nÃ¼n:
- ğŸ“± Bir **hesap makinesi**: Girdi â†’ Ä°ÅŸlem â†’ Ã‡Ä±ktÄ± (hafÄ±za yok)
- ğŸ° Bir veritabanÄ± **DEÄÄ°L**: KonuÅŸmalarÄ±nÄ±zÄ± SAKLAMAZ
- ğŸ”„ HatÄ±rlayan bir ÅŸey **DEÄÄ°L**: Her API Ã§aÄŸrÄ±sÄ± baÄŸÄ±msÄ±zdÄ±r

### Durumsuz GerÃ§eklik

```python
# Ã–rnek 1: Ä°lk konuÅŸma
response = llm.chat("AdÄ±m John")
# LLM: "TanÄ±ÅŸtÄ±ÄŸÄ±mÄ±za memnun oldum, John!"

# Ã–rnek 2: Yeni konuÅŸma (durumsuz!)
response = llm.chat("AdÄ±m ne?")
# LLM: "AdÄ±nÄ± bilmiyorum. Bana sÃ¶ylemedin."

# NEDEN? Ã‡Ã¼nkÃ¼ LLM Ã–rnek 1'i "hatÄ±rlamadÄ±"!
# Her Ã§aÄŸrÄ± baÄŸÄ±msÄ±zdÄ±r. Depolama yok. HafÄ±za yok.
```

### NasÄ±l "HatÄ±rlanÄ±r": KonuÅŸma GeÃ§miÅŸini GÃ¶nderin

```python
# DoÄŸru yol: TÃ¼m konuÅŸma geÃ§miÅŸini gÃ¶nderin
messages = [
    {"role": "user", "content": "AdÄ±m John"},
    {"role": "assistant", "content": "TanÄ±ÅŸtÄ±ÄŸÄ±mÄ±za memnun oldum, John!"},
    {"role": "user", "content": "AdÄ±m ne?"}
]
response = llm.chat(messages)
# LLM: "AdÄ±n John!"

# LLM "hatÄ±rlamaz" - her seferinde TÃœM mesajlarÄ± gÃ¶rÃ¼r!
```

---

## ğŸ“š Bu BÃ¶lÃ¼m Neleri KapsÄ±yor

### Bu Dizindeki Dosyalar

```
00-llm-basics/
â”œâ”€â”€ README.md                    â† BuradasÄ±nÄ±z
â”œâ”€â”€ requirements.txt             â† Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ 01_basic_chat.py            â† Basit sohbet Ã¶rneÄŸi
â”œâ”€â”€ 02_streaming_chat.py        â† AkÄ±ÅŸ yanÄ±tlarÄ±
â”œâ”€â”€ 03_conversation_history.py  â† BaÄŸlam yÃ¶netimi
â”œâ”€â”€ 04_system_prompts.py        â† DavranÄ±ÅŸÄ± kontrol etme
â”œâ”€â”€ 05_curl_examples.sh         â† HTTP katmanÄ± Ã¶rnekleri
â””â”€â”€ theory.md                   â† Derinlemesine inceleme: LLM'ler nasÄ±l Ã§alÄ±ÅŸÄ±r
```

### Ä°lerleme

1. **01_basic_chat.py** - MÃ¼mkÃ¼n olan en basit LLM Ã§aÄŸrÄ±sÄ±
2. **02_streaming_chat.py** - GerÃ§ek zamanlÄ± yanÄ±tlar (daha iyi UX)
3. **03_conversation_history.py** - BaÄŸlamÄ± sÃ¼rdÃ¼rme
4. **04_system_prompts.py** - LLM davranÄ±ÅŸÄ±nÄ± kontrol etme
5. **05_curl_examples.sh** - HTTP katmanÄ±nÄ± anlama

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Ollama Kurulumu

```bash
# https://ollama.ai adresini ziyaret edin ve iÅŸletim sisteminiz iÃ§in indirin
# Veya Linux'ta:
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Bir Model Ã‡ekin

```bash
# qwen2.5:3b kullanÄ±yoruz - hÄ±zlÄ±, hafif, iyi kalite
ollama pull qwen2.5:3b

# Ã‡alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrulayÄ±n
ollama list
```

### 3. Ollama'yÄ± Test Edin

```bash
# Basit sohbet testi
ollama run qwen2.5:3b "Merhaba! 2+2 kaÃ§tÄ±r?"

# API testi
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:3b",
  "prompt": "Merhaba!"
}'
```

### 4. Python BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± YÃ¼kleyin

```bash
cd 00-llm-basics
pip install -r requirements.txt
```

### 5. Ä°lk Ã–rneÄŸi Ã‡alÄ±ÅŸtÄ±rÄ±n

```bash
python 01_basic_chat.py
```

---

## ğŸ“– AyrÄ±ntÄ±lÄ± Ã–rnekler

### Ã–rnek 1: Temel Sohbet (01_basic_chat.py)

**Ne Ã–ÄŸreneceksiniz:**
- Basit bir LLM API Ã§aÄŸrÄ±sÄ± yapma
- Ä°stek/yanÄ±t yapÄ±sÄ±nÄ± anlama
- Her Ã§aÄŸrÄ±nÄ±n neden baÄŸÄ±msÄ±z (durumsuz) olduÄŸu

**Ana Kod:**
```python
import requests

# Bir LLM Ã§aÄŸrÄ±sÄ± SADECE budur: HTTP POST isteÄŸi!
response = requests.post('http://localhost:11434/api/chat', json={
    "model": "qwen2.5:3b",
    "messages": [{"role": "user", "content": "Merhaba!"}],
    "stream": false
})

# LLM, Ã¼retilen metinle birlikte JSON dÃ¶ndÃ¼rÃ¼r
print(response.json()['message']['content'])
```

**curl EÅŸdeÄŸeri:**
```bash
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:3b",
  "messages": [{"role": "user", "content": "Merhaba!"}],
  "stream": false
}'
```

**Neler Oluyor:**
1. MesajÄ±nÄ±zla birlikte HTTP POST gÃ¶nderirsiniz
2. Ollama'nÄ±n LLM'i metni iÅŸler
3. LLM bir yanÄ±t Ã¼retir (sonraki tokenlarÄ± tahmin eder)
4. Ollama JSON yanÄ±tÄ± dÃ¶ndÃ¼rÃ¼r
5. **HiÃ§bir yerde depolama olmaz!**

---

### Ã–rnek 2: AkÄ±ÅŸ YanÄ±tlarÄ± (02_streaming_chat.py)

**Ne Ã–ÄŸreneceksiniz:**
- AkÄ±ÅŸ neden kullanÄ±cÄ± deneyimini iyileÅŸtirir
- AkÄ±ÅŸ yanÄ±tlarÄ±nÄ± nasÄ±l iÅŸlersiniz
- Token-token Ã¼retim

**Ana Kod:**
```python
# Tam yanÄ±tÄ± beklemek yerine, Ã¼retildikÃ§e tokenlarÄ± akÄ±tÄ±n
response = requests.post('http://localhost:11434/api/chat', json={
    "model": "qwen2.5:3b",
    "messages": [{"role": "user", "content": "Bir ÅŸiir yaz"}],
    "stream": true  # AkÄ±ÅŸÄ± etkinleÅŸtir!
}, stream=True)

# Her token geldiÄŸinde yazdÄ±rÄ±n
for line in response.iter_lines():
    if line:
        chunk = json.loads(line)
        print(chunk['message']['content'], end='', flush=True)
```

**Neden AkÄ±ÅŸ?**
- âš¡ KullanÄ±cÄ± yanÄ±tÄ± hemen gÃ¶rÃ¼r
- ğŸ¯ Uzun yanÄ±tlar iÃ§in daha iyi UX
- ğŸ“Š Beklemek yerine ilerlemeyi gÃ¶sterir

---

### Ã–rnek 3: KonuÅŸma GeÃ§miÅŸi (03_conversation_history.py)

**Ne Ã–ÄŸreneceksiniz:**
- Birden fazla turda baÄŸlamÄ± nasÄ±l sÃ¼rdÃ¼rÃ¼rsÃ¼nÃ¼z
- KonuÅŸma geÃ§miÅŸini yÃ¶netme
- BaÄŸlam sÄ±nÄ±rlarÄ±nÄ± anlama

**Ana Kod:**
```python
# KonuÅŸmayÄ± bir listede tutun
conversation = []

# Tur 1
conversation.append({"role": "user", "content": "En sevdiÄŸim renk mavi"})
response = llm_call(conversation)
conversation.append({"role": "assistant", "content": response})

# Tur 2 - LLM "hatÄ±rlar" Ã§Ã¼nkÃ¼ tam geÃ§miÅŸi gÃ¶nderiyoruz!
conversation.append({"role": "user", "content": "En sevdiÄŸim renk ne?"})
response = llm_call(conversation)  # LLM TÃœM mesajlarÄ± gÃ¶rÃ¼r
# YanÄ±t: "En sevdiÄŸim renk mavi!"
```

**Ã–nemli:**
- LLM her seferinde **tÃ¼m konuÅŸmayÄ±** gÃ¶rÃ¼r
- GeÃ§miÅŸi yÃ¶netmekten siz sorumlusunuz
- Daha fazla geÃ§miÅŸ = daha fazla token = daha yavaÅŸ/daha pahalÄ±

---

### Ã–rnek 4: Sistem PromptlarÄ± (04_system_prompts.py)

**Ne Ã–ÄŸreneceksiniz:**
- Sistem promptlarÄ± ile LLM davranÄ±ÅŸÄ±nÄ± kontrol etme
- Ã–zelleÅŸtirilmiÅŸ asistanlar oluÅŸturma
- Prompt mÃ¼hendisliÄŸi temelleri

**Ana Kod:**
```python
messages = [
    {
        "role": "system",
        "content": "Sen yardÄ±mcÄ± bir korsan asistansÄ±n. Her zaman bir korsan gibi cevap ver!"
    },
    {
        "role": "user",
        "content": "Hava nasÄ±l?"
    }
]

# LLM cevap verecek: "Arrr! Hava verilerine eriÅŸimim yok, ahbap!"
```

**Sistem Prompt KullanÄ±m DurumlarÄ±:**
- ğŸ‘¨â€ğŸ’¼ MÃ¼ÅŸteri destek botu (arkadaÅŸ canlÄ±sÄ±, profesyonel)
- ğŸ‘¨â€ğŸ’» Kod asistanÄ± (teknik, Ã¶zlÃ¼)
- ğŸ‘¨â€ğŸ« EÄŸitmen (eÄŸitici, sabÄ±rlÄ±)
- ğŸ´â€â˜ ï¸ YaratÄ±cÄ± kiÅŸilikler (korsan, Shakespeare vb.)

---

### Ã–rnek 5: curl Ã–rnekleri (05_curl_examples.sh)

**Ne Ã–ÄŸreneceksiniz:**
- TÃ¼m yapay zeka frameworklerinin altÄ±ndaki ham HTTP katmanÄ±
- LLM'leri herhangi bir sisteme (sadece Python deÄŸil) nasÄ±l entegre edersiniz
- Ä°stek/yanÄ±t yapÄ±sÄ±

**Ã–rnekler:**

```bash
# 1. Temel sohbet
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:3b",
  "messages": [
    {"role": "user", "content": "Merhaba!"}
  ],
  "stream": false
}'

# 2. Sistem promptu ile
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:3b",
  "messages": [
    {"role": "system", "content": "Sen yardÄ±mcÄ± bir asistansÄ±n"},
    {"role": "user", "content": "Merhaba!"}
  ],
  "stream": false
}'

# 3. GeÃ§miÅŸli konuÅŸma
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:3b",
  "messages": [
    {"role": "user", "content": "AdÄ±m John"},
    {"role": "assistant", "content": "TanÄ±ÅŸtÄ±ÄŸÄ±mÄ±za memnun oldum, John!"},
    {"role": "user", "content": "AdÄ±m ne?"}
  ],
  "stream": false
}'
```

**Bu Neden Ã–nemli:**
- Herhangi bir dil/araÃ§ bu API'leri Ã§aÄŸÄ±rabilir (JavaScript, Java, Go vb.)
- Postman, Insomnia veya herhangi bir HTTP istemcisi kullanabilirsiniz
- LangChain gibi frameworklerin arka planda ne yaptÄ±ÄŸÄ±nÄ± anlayÄ±n

---

## ğŸ§© Teori Derinlemesine Ä°nceleme

### LLM'ler NasÄ±l "DÃ¼ÅŸÃ¼nÃ¼r"?

Åunlar hakkÄ±nda derinlemesine bilgi iÃ§in [theory.md](./theory.md) dosyasÄ±nÄ± okuyun:
- Token tahmini ve olasÄ±lÄ±k
- Dikkat mekanizmalarÄ± (basitleÅŸtirilmiÅŸ)
- BaÄŸlam penceresi boyutu neden Ã¶nemlidir
- Ã‡Ä±karÄ±m (inference) ve eÄŸitim (training) arasÄ±ndaki fark
- Fine-tuning neden kalÄ±cÄ±dÄ±r, promptlama geÃ§icidir

**HÄ±zlÄ± Ã–zet:**
```
EÄŸitim/Fine-tuning:
- Model aÄŸÄ±rlÄ±klarÄ±nÄ± deÄŸiÅŸtirir (kalÄ±cÄ±)
- GPU, zaman, veri gerektirir
- Detaylar iÃ§in ../fine-tuning/ bakÄ±n

Promptlama (burada yaptÄ±ÄŸÄ±mÄ±z):
- GeÃ§ici davranÄ±ÅŸ deÄŸiÅŸikliÄŸi
- Sadece farklÄ± metin gÃ¶nderme
- Model deÄŸiÅŸikliÄŸi yok
```

---

## ğŸ¯ Ana Ã‡Ä±karÄ±mlar

### Åimdi Neleri AnlamalÄ±sÄ±nÄ±z

1. **LLM'ler durumsuzdur**
   - API Ã§aÄŸrÄ±larÄ± arasÄ±nda hafÄ±za yoktur
   - Her Ã§aÄŸrÄ± baÄŸÄ±msÄ±zdÄ±r
   - KonuÅŸma geÃ§miÅŸini siz yÃ¶netirsiniz

2. **LLM'ler verilerinizi saklamaz**
   - Girdiyi iÅŸler ve Ã§Ä±ktÄ± Ã¼retir
   - Bir veritabanÄ± deÄŸil, hesap makinesi gibidir
   - "HatÄ±rlamak" iÃ§in konuÅŸma geÃ§miÅŸini kullanÄ±n

3. **HTTP katmanÄ± basittir**
   - JSON ile POST isteÄŸi
   - LLM JSON yanÄ±tÄ± dÃ¶ndÃ¼rÃ¼r
   - Herhangi bir dilden/araÃ§tan Ã§aÄŸrÄ±labilir

4. **Sistem promptlarÄ± davranÄ±ÅŸÄ± kontrol eder**
   - LLM'in nasÄ±l davranmasÄ± gerektiÄŸini tanÄ±mlayÄ±n
   - GeÃ§ici (sadece o konuÅŸma iÃ§in)
   - Fine-tuning gibi kalÄ±cÄ± deÄŸil

5. **AkÄ±ÅŸ UX'i iyileÅŸtirir**
   - Tokenlar Ã¼retildikÃ§e gelir
   - KullanÄ±cÄ± anÄ±nda ilerlemeyi gÃ¶rÃ¼r
   - Uzun yanÄ±tlar iÃ§in daha iyi

---

## ğŸš€ Sonraki AdÄ±mlar

### Åuna HazÄ±rsÄ±nÄ±z:
âœ… [01-tool-calling](../01-tool-calling) - LLM'lere fonksiyon Ã§aÄŸÄ±rma ile sÃ¼per gÃ¼Ã§ler verin

### Devam Etmeden Ã–nce:

Bu kendi kendine testi Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
# Bunun neden olduÄŸunu aÃ§Ä±klayabilir misiniz?
ollama run qwen2.5:3b "AdÄ±m Alice"
# YanÄ±t: "Merhaba Alice!"

ollama run qwen2.5:3b "AdÄ±m ne?"
# YanÄ±t: "AdÄ±nÄ± bilmiyorum"

# Cevap: Ã‡Ã¼nkÃ¼ her ollama Ã§alÄ±ÅŸtÄ±rmasÄ± ayrÄ±,
# baÄŸÄ±msÄ±z bir API Ã§aÄŸrÄ±sÄ±dÄ±r. PaylaÅŸÄ±lan durum yok!
```

---

## ğŸ› Hata AyÄ±klama Ä°puÃ§larÄ±

### YaygÄ±n Sorunlar

**1. "Connection refused" hatasÄ±**
```bash
# Ã‡Ã¶zÃ¼m: Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
ollama serve  # Ollama sunucusunu baÅŸlat
```

**2. "Model not found"**
```bash
# Ã‡Ã¶zÃ¼m: Ã–nce modeli Ã§ekin
ollama pull qwen2.5:3b
```

**3. "Response is slow"**
```bash
# Ã‡Ã¶zÃ¼m: Daha iyi algÄ±lanan hÄ±z iÃ§in akÄ±ÅŸ kullanÄ±n
# Veya daha kÃ¼Ã§Ã¼k bir model kullanÄ±n
ollama pull qwen2.5:1.5b  # Daha kÃ¼Ã§Ã¼k = daha hÄ±zlÄ±
```

**4. "Context too long"**
- Eski konuÅŸma geÃ§miÅŸini kÄ±rpÄ±n
- Her modelin bir baÄŸlam sÄ±nÄ±rÄ± vardÄ±r (genellikle 2048-8192 token)
- 1 token â‰ˆ 0.75 kelime

---

## ğŸ“š Ek Kaynaklar

- [Ollama DokÃ¼mantasyonu](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [OpenAI API ReferansÄ±](https://platform.openai.com/docs/api-reference) (benzer yapÄ±)
- [theory.md](./theory.md) - LLM iÃ§ yapÄ±sÄ±na derinlemesine bakÄ±ÅŸ

---

**SÄ±radaki:** [01-tool-calling](../01-tool-calling) - Fonksiyon Ã§aÄŸÄ±rma ile LLM'lere nasÄ±l sÃ¼per gÃ¼Ã§ler vereceÄŸinizi Ã¶ÄŸrenin â†’
