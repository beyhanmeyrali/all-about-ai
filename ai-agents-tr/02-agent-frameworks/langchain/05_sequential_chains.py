#!/usr/bin/env python3
"""
Ã–rnek 05: SÄ±ralÄ± Zincirler - Ã‡ok AdÄ±mlÄ± Ä°ÅŸ AkÄ±ÅŸlarÄ±
=====================================================

KarmaÅŸÄ±k iÅŸ akÄ±ÅŸlarÄ± iÃ§in birden fazla LLM Ã§aÄŸrÄ±sÄ±nÄ± nasÄ±l zincirleneceÄŸini Ã¶ÄŸrenin!

Ne Ã¶ÄŸreneceksiniz:
- SÄ±ralÄ± iÅŸleme (adÄ±m1 â†’ adÄ±m2 â†’ adÄ±m3)
- AdÄ±mlar arasÄ±nda veri aktarÄ±mÄ±
- LCEL (LangChain Ä°fade Dili) - Modern yaklaÅŸÄ±m
- Veri iÅŸleme iÃ§in dÃ¶nÃ¼ÅŸÃ¼m zincirleri
- Ãœretim boru hattÄ± desenleri

GERÃ‡EK Ã§ok adÄ±mlÄ± ajanlarÄ± bu ÅŸekilde oluÅŸturursunuz!

Yazar: Beyhan MEYRALI
"""

from typing import Dict, Any, List
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


# =============================================================================
# BÃ–LÃœM 1: Basit SÄ±ralÄ± Zincir (LCEL Stili)
# =============================================================================

class SimpleSequentialPipeline:
    """
    LCEL (LangChain Ä°fade Dili) kullanarak basit sÄ±ralÄ± boru hattÄ±.

    Bu, LangChain 1.1.0+'da iÅŸlemleri zincirlemenin MODERN yoludur.
    BileÅŸenleri zincirleme iÃ§in boru operatÃ¶rÃ¼nÃ¼ (|) kullanÄ±r.
    """

    def __init__(self, model: str = "qwen3:8b"):
        """Boru hattÄ±nÄ± baÅŸlat."""
        print(f"\n[BAÅLAT] {model} ile SimpleSequentialPipeline oluÅŸturuluyor...")
        self.llm = OllamaLLM(model=model, temperature=0.7)
        print("[BAÅLAT] âœ… Boru hattÄ± hazÄ±r!")

    def create_story_pipeline(self):
        """
        2 adÄ±mlÄ± bir boru hattÄ± oluÅŸtur: Fikir Ã¼ret â†’ Hikaye yaz

        DÃ¶ndÃ¼rÃ¼r:
            Ã‡alÄ±ÅŸtÄ±rÄ±labilir zincir
        """
        print("\n[BORU HATTI] Hikaye oluÅŸturma boru hattÄ± inÅŸa ediliyor...")

        # AdÄ±m 1: Hikaye fikri Ã¼ret
        idea_prompt = PromptTemplate.from_template(
            "Bir cÃ¼mlede yaratÄ±cÄ± bir {genre} hikaye fikri Ã¼ret."
        )

        # AdÄ±m 2: Fikri tam hikayeye geniÅŸlet
        story_prompt = PromptTemplate.from_template(
            "Bu hikaye fikrini al ve 3 paragraflÄ±k kÄ±sa bir hikaye yaz:\n\n{idea}"
        )

        # LCEL kullanarak zincirle (boru operatÃ¶rÃ¼)
        # idea_prompt | llm â†’ fikir Ã¼retir
        # Sonra bunu story_prompt | llm'e aktar â†’ tam hikaye Ã¼retir

        chain = (
            idea_prompt
            | self.llm
            | (lambda idea: {"idea": idea})
            | story_prompt
            | self.llm
            | StrOutputParser()
        )

        print("[BORU HATTI] âœ… Boru hattÄ±: idea_prompt | llm | story_prompt | llm")
        return chain

    def run(self, genre: str) -> str:
        """Boru hattÄ±nÄ± Ã§alÄ±ÅŸtÄ±r."""
        print(f"\n[Ã‡ALIÅTIR] {genre} hikayesi oluÅŸturuluyor...")

        chain = self.create_story_pipeline()
        result = chain.invoke({"genre": genre})

        print(f"\n[SONUÃ‡] Hikaye oluÅŸturuldu!")
        return result


# =============================================================================
# BÃ–LÃœM 2: Ã‡ok AdÄ±mlÄ± Veri Ä°ÅŸleme Boru HattÄ±
# =============================================================================

class DataProcessingPipeline:
    """
    Veri Ã§Ä±karma ve iÅŸleme iÃ§in Ã§ok adÄ±mlÄ± boru hattÄ±.

    AdÄ±mlar: Ã‡Ä±kar â†’ Analiz Et â†’ Ã–zetle
    """

    def __init__(self, model: str = "qwen3:8b"):
        """Boru hattÄ±nÄ± baÅŸlat."""
        self.llm = OllamaLLM(model=model, temperature=0.3)

    def create_analysis_pipeline(self):
        """
        3 adÄ±mlÄ± analiz boru hattÄ± oluÅŸtur.

        AdÄ±m 1: Ana noktalarÄ± Ã§Ä±kar
        AdÄ±m 2: Duyguyu analiz et
        AdÄ±m 3: Ã–zet oluÅŸtur
        """

        # AdÄ±m 1: Ana noktalarÄ± Ã§Ä±kar
        extract_prompt = PromptTemplate.from_template(
            """Bu metinden ana noktalarÄ± madde listesi olarak Ã§Ä±kar:

Metin: {text}

Ana Noktalar:"""
        )

        # AdÄ±m 2: Duyguyu analiz et
        sentiment_prompt = PromptTemplate.from_template(
            """Bu ana noktalarÄ±n duygusunu analiz et:

{key_points}

Duygu (olumlu/olumsuz/nÃ¶tr):"""
        )

        # AdÄ±m 3: Ã–zet oluÅŸtur
        summary_prompt = PromptTemplate.from_template(
            """Final Ã¶zet oluÅŸtur:

Ana Noktalar: {key_points}
Duygu: {sentiment}

Ã–zet:"""
        )

        # Boru hattÄ±nÄ± oluÅŸtur
        def extract_step(input_dict):
            """AdÄ±m 1: Ana noktalarÄ± Ã§Ä±kar."""
            result = (extract_prompt | self.llm).invoke(input_dict)
            return {"key_points": result, "text": input_dict["text"]}

        def sentiment_step(input_dict):
            """AdÄ±m 2: Duyguyu analiz et."""
            result = (sentiment_prompt | self.llm).invoke(input_dict)
            return {
                "key_points": input_dict["key_points"],
                "sentiment": result
            }

        def summary_step(input_dict):
            """AdÄ±m 3: Ã–zet oluÅŸtur."""
            result = (summary_prompt | self.llm | StrOutputParser()).invoke(input_dict)
            return result

        # BunlarÄ± birbirine zincirle
        from langchain_core.runnables import RunnableLambda

        chain = (
            RunnableLambda(extract_step)
            | RunnableLambda(sentiment_step)
            | RunnableLambda(summary_step)
        )

        return chain

    def analyze(self, text: str) -> str:
        """Metni boru hattÄ±ndan geÃ§irerek analiz et."""
        print("\n[BORU HATTI] 3 adÄ±mlÄ± analiz Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        print("  AdÄ±m 1: Ana noktalar Ã§Ä±karÄ±lÄ±yor...")
        print("  AdÄ±m 2: Duygu analiz ediliyor...")
        print("  AdÄ±m 3: Ã–zet oluÅŸturuluyor...")

        chain = self.create_analysis_pipeline()
        result = chain.invoke({"text": text})

        return result


# =============================================================================
# BÃ–LÃœM 3: Hata YÃ¶netimli Ãœretim Boru HattÄ±
# =============================================================================

class ProductionPipeline:
    """
    Hata yÃ¶netimi ve gÃ¼nlÃ¼kleme ile Ã¼retim sÄ±nÄ±fÄ± boru hattÄ±.
    """

    def __init__(self, model: str = "qwen3:8b"):
        """Ãœretim boru hattÄ±nÄ± baÅŸlat."""
        self.llm = OllamaLLM(model=model, temperature=0.5)

    def create_content_pipeline(self):
        """
        Ä°Ã§erik oluÅŸturma boru hattÄ±: AraÅŸtÄ±r â†’ Taslak â†’ Yaz â†’ DÃ¼zenle
        """

        # AdÄ±m 1: AraÅŸtÄ±r
        research_prompt = PromptTemplate.from_template(
            """'{topic}' konusunu araÅŸtÄ±r ve 3 ana gerÃ§eÄŸi listele.

GerÃ§ekler:"""
        )

        # AdÄ±m 2: Taslak oluÅŸtur
        outline_prompt = PromptTemplate.from_template(
            """Bu gerÃ§eklere dayanarak bir blog yazÄ±sÄ± taslaÄŸÄ± oluÅŸtur:

{facts}

Taslak:"""
        )

        # AdÄ±m 3: Ä°Ã§erik yaz
        write_prompt = PromptTemplate.from_template(
            """Bu taslaÄŸÄ± takip ederek bir blog yazÄ±sÄ± yaz:

{outline}

Blog YazÄ±sÄ±:"""
        )

        # Hata yÃ¶netimiyle boru hattÄ± oluÅŸtur
        def safe_step(prompt, step_name):
            """GÃ¼venli yÃ¼rÃ¼tme iÃ§in sarmalayÄ±cÄ±."""
            def execute(input_dict):
                try:
                    print(f"  [{step_name}] Ä°ÅŸleniyor...")
                    result = (prompt | self.llm).invoke(input_dict)
                    print(f"  [{step_name}] âœ… TamamlandÄ±")
                    return result
                except Exception as e:
                    print(f"  [{step_name}] âŒ Hata: {e}")
                    return f"{step_name}'de hata: {str(e)}"
            return execute

        # Hata yÃ¶netimiyle zincirle
        from langchain_core.runnables import RunnableLambda

        chain = (
            RunnableLambda(lambda x: {"topic": x["topic"]})
            | RunnableLambda(safe_step(research_prompt, "ARAÅTIRMA"))
            | (lambda facts: {"facts": facts})
            | RunnableLambda(safe_step(outline_prompt, "TASLAK"))
            | (lambda outline: {"outline": outline})
            | RunnableLambda(safe_step(write_prompt, "YAZMA"))
            | StrOutputParser()
        )

        return chain

    def create_content(self, topic: str) -> str:
        """Boru hattÄ± aracÄ±lÄ±ÄŸÄ±yla iÃ§erik oluÅŸtur."""
        print(f"\n[ÃœRETÄ°M] Ä°Ã§erik oluÅŸturuluyor: {topic}")

        chain = self.create_content_pipeline()
        result = chain.invoke({"topic": topic})

        return result


# =============================================================================
# DEMOLAR
# =============================================================================

def demo_simple_sequential():
    """Demo: Basit 2 adÄ±mlÄ± boru hattÄ±."""
    print("\n" + "="*70)
    print("DEMO 1: Basit SÄ±ralÄ± Boru HattÄ±")
    print("="*70)

    pipeline = SimpleSequentialPipeline()
    story = pipeline.run("bilim kurgu")

    print("\n[HÄ°KAYE]:")
    print("-" * 70)
    print(story)
    print("-" * 70)


def demo_data_processing():
    """Demo: Ã‡ok adÄ±mlÄ± veri iÅŸleme."""
    print("\n" + "="*70)
    print("DEMO 2: Veri Ä°ÅŸleme Boru HattÄ±")
    print("="*70)

    text = """
    Yeni yapay zeka Ã¼rÃ¼n lansmanÄ± bÃ¼yÃ¼k bir baÅŸarÄ±ydÄ±! MÃ¼ÅŸteri geri bildirimleri
    son derece olumlu oldu. SatÄ±ÅŸlar beklentileri %150 aÅŸtÄ±. Ekip
    inanÄ±lmaz derecede Ã§ok Ã§alÄ±ÅŸtÄ± ve harika bir sonuÃ§ verdi. BazÄ± kÃ¼Ã§Ã¼k hatalar
    bildirildi ancak hÄ±zlÄ±ca dÃ¼zeltildi.
    """

    pipeline = DataProcessingPipeline()
    summary = pipeline.analyze(text)

    print("\n[Ã–ZET]:")
    print("-" * 70)
    print(summary)
    print("-" * 70)


def demo_production_pipeline():
    """Demo: Ãœretim boru hattÄ±."""
    print("\n" + "="*70)
    print("DEMO 3: Ãœretim Ä°Ã§erik Boru HattÄ±")
    print("="*70)

    pipeline = ProductionPipeline()
    content = pipeline.create_content("Yapay zeka ajanlarÄ±nÄ±n faydalarÄ±")

    print("\n[Ä°Ã‡ERÄ°K]:")
    print("-" * 70)
    print(content)
    print("-" * 70)


def main():
    """Ana giriÅŸ noktasÄ±."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Ã–rnek 05: SÄ±ralÄ± Zincirler                                â•‘
â•‘                                                                   â•‘
â•‘  Bu ÅŸunlarÄ± gÃ¶sterir:                                            â•‘
â•‘  â€¢ LCEL (LangChain Ä°fade Dili) - Modern yaklaÅŸÄ±m               â•‘
â•‘  â€¢ | operatÃ¶rÃ¼ ile iÅŸlemleri borulama                           â•‘
â•‘  â€¢ Ã‡ok adÄ±mlÄ± sÄ±ralÄ± iÅŸ akÄ±ÅŸlarÄ±                                â•‘
â•‘  â€¢ AdÄ±mlar arasÄ±nda veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼                               â•‘
â•‘  â€¢ Ãœretim boru hattÄ± desenleri                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # DemolarÄ± Ã§alÄ±ÅŸtÄ±r
    demo_simple_sequential()
    demo_data_processing()
    demo_production_pipeline()

    # Ã–zet
    print("\n" + "="*70)
    print("âœ… TAMAMLANDI!")
    print("="*70)
    print("\nğŸ“ Ne Ã¶ÄŸrendiniz:")
    print("  1. LCEL (LangChain Ä°fade Dili) - Modern zincirleme")
    print("  2. SÄ±ralÄ± iÅŸlemler iÃ§in boru operatÃ¶rÃ¼ (|)")
    print("  3. Ã‡ok adÄ±mlÄ± iÅŸ akÄ±ÅŸlarÄ± (Ã§Ä±kar â†’ analiz et â†’ Ã¶zetle)")
    print("  4. Zincir adÄ±mlarÄ± arasÄ±nda veri aktarÄ±mÄ±")
    print("  5. Hata yÃ¶netimiyle Ã¼retim desenleri")
    print("\nğŸ“– Temel Kavramlar:")
    print("  â€¢ SÄ±ralÄ± = Birbiri ardÄ±na bir adÄ±m")
    print("  â€¢ LCEL = Modern LangChain zincirleme (kullanÄ±mdan kaldÄ±rÄ±lmadÄ±)")
    print("  â€¢ Boru (|) = BileÅŸenleri birbirine baÄŸla")
    print("  â€¢ DÃ¶nÃ¼ÅŸÃ¼m = AdÄ±mlar arasÄ±nda veriyi deÄŸiÅŸtir")
    print("\nğŸ’¡ LCEL vs Eski SequentialChain:")
    print("  ESKÄ° (kullanÄ±mdan kaldÄ±rÄ±ldÄ±): SequentialChain([chain1, chain2])")
    print("  YENÄ° (LCEL):                   prompt | llm | parser")
    print("  LCEL daha basit, daha esnek ve kullanÄ±mdan kaldÄ±rÄ±lmadÄ±!")
    print("\nâ¡ï¸  SÄ±radaki: python 06_router_chains.py")
    print("="*70)


if __name__ == "__main__":
    import requests
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            print("[UYARI] Ollama doÄŸru Ã§alÄ±ÅŸmÄ±yor olabilir")
    except:
        print("[HATA] Ollama'ya baÄŸlanÄ±lamÄ±yor!")
        print("  Ã‡Ã¶zÃ¼m: ollama serve")
        exit(1)

    main()
