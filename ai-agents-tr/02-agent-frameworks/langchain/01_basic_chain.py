#!/usr/bin/env python3
"""
Ã–rnek 01: Temel LangChain - Ä°lk Zinciriniz
===============================================

Bu, mÃ¼mkÃ¼n olan EN BASÄ°T LangChain Ã¶rneÄŸidir.
Temel kavramlarÄ± Ã¶ÄŸrenin: LLM, Prompt, Zincir.

Ne Ã¶ÄŸreneceksiniz:
- Bir LLM Ã¶rneÄŸi nasÄ±l oluÅŸturulur
- Basit bir prompt nasÄ±l oluÅŸturulur
- Bunlar nasÄ±l birbirine zincirlenir
- Zincir nasÄ±l Ã§aÄŸrÄ±lÄ±r

Bu, LangChain iÃ§in "Merhaba DÃ¼nya"nÄ±zdÄ±r!

HATA AYIKLAMA Ä°PUÃ‡LARI:
--------------
1. OllamaLLM bulunamazsa:
   pip install langchain-ollama

2. BaÄŸlantÄ± baÅŸarÄ±sÄ±z olursa:
   - Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin: ollama serve
   - Modelin var olduÄŸunu kontrol edin: ollama list

3. Ne olduÄŸunu gÃ¶rmek iÃ§in:
   - LLMChain'de verbose=True ayarlayÄ±n
   - print() ifadeleri ekleyin

Yazar: Beyhan MEYRALI
"""

from typing import Dict, Any
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


class BasicChainAgent:
    """
    SorularÄ± yanÄ±tlayan basit bir LangChain ajanÄ±.

    Bu, en temel LangChain desenini gÃ¶sterir:
    Prompt â†’ LLM â†’ YanÄ±t

    Ã–zellikler:
        llm: Dil modeli Ã¶rneÄŸi
        prompt_template: PromptlarÄ± biÃ§imlendirmek iÃ§in ÅŸablon
        chain: Her ÅŸeyi bir araya getiren LLMChain
    """

    def __init__(self, model: str = "qwen3:8b", temperature: float = 0.7):
        """
        Temel zincir ajanÄ±nÄ± baÅŸlat.

        ArgÃ¼manlar:
            model: Ollama model adÄ±
            temperature: LLM sÄ±caklÄ±ÄŸÄ± (0.0 = deterministik, 1.0 = yaratÄ±cÄ±)
        """
        print(f"\n[BAÅLAT] {model} ile BasicChainAgent oluÅŸturuluyor...")

        # AdÄ±m 1: LLM Ã¶rneÄŸi oluÅŸtur
        self.llm = self._create_llm(model, temperature)

        # AdÄ±m 2: Prompt ÅŸablonu oluÅŸtur
        self.prompt_template = self._create_prompt_template()

        # AdÄ±m 3: Zinciri oluÅŸtur
        self.chain = self._create_chain()

        print("[BAÅLAT] âœ… Ajan baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!")

    def _create_llm(self, model: str, temperature: float) -> OllamaLLM:
        """
        Bir Ollama LLM Ã¶rneÄŸi oluÅŸtur.

        Bu, Ollama ile konuÅŸan Ã§ekirdek bileÅŸendir.

        ArgÃ¼manlar:
            model: Model adÄ±
            temperature: YaratÄ±cÄ±lÄ±k seviyesi

        DÃ¶ndÃ¼rÃ¼r:
            YapÄ±landÄ±rÄ±lmÄ±ÅŸ OllamaLLM Ã¶rneÄŸi
        """
        print(f"  LLM oluÅŸturuluyor: {model} (temperature={temperature})")

        llm = OllamaLLM(
            model=model,
            temperature=temperature,
            # base_url="http://localhost:11434",  # VarsayÄ±lan, Ã¶zelleÅŸtirilebilir
        )

        return llm

    def _create_prompt_template(self) -> PromptTemplate:
        """
        Bir prompt ÅŸablonu oluÅŸtur.

        Åablonlar, farklÄ± girdilerle promptlarÄ± yeniden kullanmamÄ±zÄ± saÄŸlar.

        DÃ¶ndÃ¼rÃ¼r:
            PromptTemplate Ã¶rneÄŸi
        """
        print("  Prompt ÅŸablonu oluÅŸturuluyor...")

        template = """YardÄ±msever bir yapay zeka asistanÄ±sÄ±n.

KullanÄ±cÄ± sorusu: {question}

LÃ¼tfen net ve Ã¶zlÃ¼ bir cevap ver."""

        prompt = PromptTemplate(
            template=template,
            input_variables=["question"]
        )

        return prompt

    def _create_chain(self):
        """
        LCEL zincirini oluÅŸtur.

        Bu, LCEL kullanarak promptu ve LLM'i yeniden kullanÄ±labilir bir zincirde birleÅŸtirir.

        DÃ¶ndÃ¼rÃ¼r:
            YapÄ±landÄ±rÄ±lmÄ±ÅŸ LCEL zinciri
        """
        print("  LCEL zinciri oluÅŸturuluyor...")

        # Modern LCEL sÃ¶zdizimi: prompt | llm | parser
        chain = self.prompt_template | self.llm | StrOutputParser()

        return chain

    def ask(self, question: str) -> str:
        """
        Bir soru sor ve cevap al.

        Bu, kullanacaÄŸÄ±nÄ±z ana metoddur.

        ArgÃ¼manlar:
            question: Sorulacak soru

        DÃ¶ndÃ¼rÃ¼r:
            LLM'in cevabÄ±
        """
        print(f"\n[SORULUYOR] {question}")

        try:
            # LCEL invoke kullanarak zinciri Ã§alÄ±ÅŸtÄ±r
            response = self.chain.invoke({"question": question})

            print(f"[CEVAP] {response[:100]}...")
            return response

        except Exception as e:
            error_msg = f"Hata: {str(e)}"
            print(f"[HATA] {error_msg}")
            return error_msg

    def ask_with_details(self, question: str) -> Dict[str, Any]:
        """
        Bir soru sor ve ayrÄ±ntÄ±lÄ± bilgi al.

        Bu, arka planda ne olduÄŸunu gÃ¶sterir.

        ArgÃ¼manlar:
            question: Sorulacak soru

        DÃ¶ndÃ¼rÃ¼r:
            Soru, cevap ve meta veri iÃ§eren sÃ¶zlÃ¼k
        """
        print(f"\n[AYRINTILI SOR] {question}")

        # Promptu biÃ§imlendir
        formatted_prompt = self.prompt_template.format(question=question)

        print(f"\n[LLM'E GÃ–NDERÄ°LEN PROMPT]:")
        print("-" * 70)
        print(formatted_prompt)
        print("-" * 70)

        # LCEL kullanarak yanÄ±t al
        response = self.chain.invoke({"question": question})

        print(f"\n[LLM'DEN GELEN YANIT]:")
        print("-" * 70)
        print(response)
        print("-" * 70)

        return {
            "question": question,
            "formatted_prompt": formatted_prompt,
            "answer": response,
            "model": "qwen3:8b",
        }


def demo_basic_usage():
    """Temel kullanÄ±mÄ± gÃ¶ster."""
    print("\n" + "="*70)
    print("DEMO 1: Temel KullanÄ±m")
    print("="*70)

    # Ajan oluÅŸtur
    agent = BasicChainAgent()

    # Basit sorular sor
    agent.ask("Fransa'nÄ±n baÅŸkenti neresidir?")
    agent.ask("15 * 7 kaÃ§tÄ±r?")
    agent.ask("ÃœÃ§ ana renk nelerdir?")


def demo_detailed_usage():
    """Ä°Ã§ yapÄ±yÄ± gÃ¶rmek iÃ§in ayrÄ±ntÄ±lÄ± kullanÄ±mÄ± gÃ¶ster."""
    print("\n" + "="*70)
    print("DEMO 2: AyrÄ±ntÄ±lÄ± KullanÄ±m (Ne OlduÄŸunu GÃ¶r)")
    print("="*70)

    agent = BasicChainAgent()

    # AyrÄ±ntÄ±larla sor
    result = agent.ask_with_details("Bir sinir aÄŸÄ±nÄ±n ne olduÄŸunu bir cÃ¼mlede aÃ§Ä±kla.")

    print("\n[SONUÃ‡ SÃ–ZLÃœÄÃœ]:")
    print(f"  Soru: {result['question']}")
    print(f"  Model: {result['model']}")
    print(f"  Cevap uzunluÄŸu: {len(result['answer'])} karakter")


def demo_different_temperatures():
    """SÄ±caklÄ±ÄŸÄ±n yanÄ±tlarÄ± nasÄ±l etkilediÄŸini gÃ¶ster."""
    print("\n" + "="*70)
    print("DEMO 3: SÄ±caklÄ±k KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    print("="*70)

    question = "Bir bilim kurgu hikayesi iÃ§in yaratÄ±cÄ± bir aÃ§Ä±lÄ±ÅŸ cÃ¼mlesi yaz."

    print(f"\nSoru: {question}\n")

    # DÃ¼ÅŸÃ¼k sÄ±caklÄ±k (deterministik)
    print("[SÄ±caklÄ±k = 0.0 - Deterministik]")
    agent_low = BasicChainAgent(temperature=0.0)
    response_low = agent_low.ask(question)

    # YÃ¼ksek sÄ±caklÄ±k (yaratÄ±cÄ±)
    print("\n[SÄ±caklÄ±k = 1.0 - YaratÄ±cÄ±]")
    agent_high = BasicChainAgent(temperature=1.0)
    response_high = agent_high.ask(question)

    print("\nğŸ’¡ SÄ±caklÄ±ÄŸÄ±n yaratÄ±cÄ±lÄ±ÄŸÄ± nasÄ±l etkilediÄŸine dikkat edin!")


def main():
    """Ana giriÅŸ noktasÄ±."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Ã–rnek 01: Temel LangChain - Ä°lk Zinciriniz               â•‘
â•‘                                                                   â•‘
â•‘  Bu ÅŸunlarÄ± gÃ¶sterir:                                            â•‘
â•‘  â€¢ Bir LLM Ã¶rneÄŸi oluÅŸturma (OllamaLLM)                         â•‘
â•‘  â€¢ Bir prompt ÅŸablonu oluÅŸturma (PromptTemplate)                 â•‘
â•‘  â€¢ BunlarÄ± birbirine zincirleme (LLMChain)                      â•‘
â•‘  â€¢ Zinciri Ã§alÄ±ÅŸtÄ±rma (chain.run)                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # DemolarÄ± Ã§alÄ±ÅŸtÄ±r
    demo_basic_usage()
    demo_detailed_usage()
    demo_different_temperatures()

    # Ã–zet
    print("\n" + "="*70)
    print("âœ… TAMAMLANDI!")
    print("="*70)
    print("\nğŸ“ Ne Ã¶ÄŸrendiniz:")
    print("  1. Bir OllamaLLM Ã¶rneÄŸi nasÄ±l oluÅŸturulur")
    print("  2. Bir PromptTemplate nasÄ±l oluÅŸturulur")
    print("  3. LLMChain ile nasÄ±l zincirlenir")
    print("  4. Zincir nasÄ±l Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r")
    print("  5. SÄ±caklÄ±k yanÄ±tlarÄ± nasÄ±l etkiler")
    print("\nğŸ“– Temel Kavramlar:")
    print("  â€¢ LLM = Dil modeli")
    print("  â€¢ Prompt = LLM'e gÃ¶nderdiÄŸiniz ÅŸey")
    print("  â€¢ Zincir = LLM + Prompt'un yeniden kullanÄ±labilir kombinasyonu")
    print("  â€¢ SÄ±caklÄ±k = YaratÄ±cÄ±lÄ±k seviyesi (0.0-1.0)")
    print("\nâ¡ï¸  SÄ±radaki: python 02_prompt_templates.py")
    print("="*70)


if __name__ == "__main__":
    # HÄ±zlÄ± kontrol
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            print("[UYARI] Ollama doÄŸru Ã§alÄ±ÅŸmÄ±yor olabilir")
    except:
        print("[HATA] Ollama'ya baÄŸlanÄ±lamÄ±yor!")
        print("  Ã‡Ã¶zÃ¼m: ollama serve")
        exit(1)

    main()
